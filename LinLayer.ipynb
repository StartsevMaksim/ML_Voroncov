{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fcc38af-8ca8-4793-87f2-6976145123ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from UNeuralNetwork import LinearLayer, LayerSequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3fe3b18-6e01-4a71-8cf1-c8ea2b5f1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Реклама\n",
    "data = pd.read_csv('DATA/Advertising.csv')\n",
    "X = data.drop('sales', axis=1)\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8d8418-212d-432c-b566-d763e9300695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 num_layers):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        cur_dim = input_dim\n",
    "        for index in range(num_layers):\n",
    "            self.layers.add_module(f'Linear{index+1}', torch.nn.Linear(cur_dim, hidden_dim, dtype=torch.float64))\n",
    "            self.layers.add_module(f'Relu{index+1}', torch.nn.ReLU())\n",
    "            cur_dim = hidden_dim\n",
    "        self.layers.add_module(f'Regression', torch.nn.Linear(cur_dim, output_dim, dtype=torch.float64))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def trainer(model, \n",
    "            X_train, \n",
    "            y_train,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            epochs=10):\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        model.zero_grad()\n",
    "        output = model(X_train).reshape(-1,)\n",
    "        loss = loss_function(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def tester(model,\n",
    "           X_test,\n",
    "           y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_predict = model(X_test).reshape(-1,)\n",
    "        print(y_predict[:5].numpy())\n",
    "        print(y_test[:5].numpy())\n",
    "        print(f'Ошибка на тестовой выборке = {root_mean_squared_error(y_test, y_predict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f98608e-cae1-4d97-a6c5-3cee41f1e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).to('cpu')\n",
    "X_test = torch.from_numpy(X_test).to('cpu')\n",
    "y_train = torch.from_numpy(y_train.values).to('cpu')\n",
    "y_test = torch.from_numpy(y_test.values).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12d8fc9-2542-42f5-aa25-7ec5131462a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.24614743 19.1124509  11.53712033 16.18959438  8.54227422]\n",
      "[14.7 19.8 11.9 16.7  9.5]\n",
      "Ошибка на тестовой выборке = 0.7105478354395045\n"
     ]
    }
   ],
   "source": [
    "model = Perceptron(3,5,1,2).to('cpu')\n",
    "\n",
    "trainer(model, \n",
    "        X_train,\n",
    "        y_train,\n",
    "        torch.optim.Adam(model.parameters()),\n",
    "        torch.nn.MSELoss(),\n",
    "        2000)\n",
    "\n",
    "tester(model,\n",
    "       X_test,\n",
    "       y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3654f17b-a2ab-4960-8d07-cbf9f72e75a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(\n",
       "  (layers): Sequential(\n",
       "    (Linear1): Linear(in_features=3, out_features=5, bias=True)\n",
       "    (Relu1): ReLU()\n",
       "    (Linear2): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (Relu2): ReLU()\n",
       "    (Regression): Linear(in_features=5, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da490a6-30c7-4478-a940-5560ba8ea2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aeafa0c7-d312-4678-a53c-946048a22059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USequential:\n",
    "    def __init__(self):\n",
    "        self.layers_name = []\n",
    "        self.layers = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ''.join([f'({layer_name}): {layer}\\n'\n",
    "                        for layer_name, layer in zip(self.layers_name, self.layers)])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.layers[index]\n",
    "    \n",
    "    def add_module(self, layer_name, layer):\n",
    "        self.layers_name.append(layer_name)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x, y, loss_function_type):\n",
    "        self.layers[-1].setErrors(self._loss_function[loss_function_type](x, y))\n",
    "        further_layer = self.layers[-1]\n",
    "        for layer in self.layers[-2::-1]:\n",
    "            layer.backward(further_layer)\n",
    "            further_layer = layer    \n",
    "\n",
    "    def step(self, lr):\n",
    "        for layer in self.layers:\n",
    "            layer.step(lr)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _dirSquareError(x, y):\n",
    "        vector = 2 * (x - y)\n",
    "        return vector / vector.numel()\n",
    "\n",
    "    _loss_function = {'MSE': _dirSquareError}\n",
    "\n",
    "\n",
    "class ULinear:\n",
    "    def __init__(self, \n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 bias=True,\n",
    "                 start_weights_type='random'):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights_ = self._start_weights[start_weights_type]((in_features, out_features))\n",
    "        self.bias_ = self._start_weights[start_weights_type]((out_features,)) if bias else torch.zeros((out_features,))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear(in_features={self.in_features}, out_features={self.out_features})'\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.prev_output_ = x.detach().clone()\n",
    "        return self.weights_ @ x + self.bias_\n",
    "\n",
    "    def backward(self, further_layer):\n",
    "        self.errors_ = further_layer._getPrevLayerErrors()\n",
    "\n",
    "    def step(self, lr):\n",
    "        self.weights_ -= lr * (self.errors_.reshape(-1,1) @ self.prev_output_.reshape(1,-1))\n",
    "\n",
    "    def _getPrevLayerErrors(self):\n",
    "        return self.errors_ @ self.weights_\n",
    "\n",
    "    def setErrors(self, errors):\n",
    "        self.errors_ = errors\n",
    "\n",
    "    _start_weights = {'random': torch.randn}\n",
    "\n",
    "\n",
    "class UReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'ReLU()'\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z_ = torch.where(x >= 0, 1, 0)\n",
    "        return torch.where(x >= 0, x, 0)\n",
    "\n",
    "    def backward(self, further_layer):\n",
    "        self.errors_ = further_layer._getPrevLayerErrors()\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass\n",
    "\n",
    "    def _getPrevLayerErrors(self):\n",
    "        return self.errors_ * self.z_\n",
    "\n",
    "    def setErrors(self, errors):\n",
    "        self.errors_ = errors\n",
    "\n",
    "class UTanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Tanh()'\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z_ = -torch.pow(torch.tanh(x), 2) + 1\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def backward(self, further_layer):\n",
    "        self.errors_ = further_layer._getPrevLayerErrors()\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass\n",
    "\n",
    "    def _getPrevLayerErrors(self):\n",
    "        return self.errors_ * self.z_\n",
    "\n",
    "    def setErrors(self, errors):\n",
    "        self.errors_ = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9e26012c-d36d-4f03-869d-269476d50875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UConv2d:\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 bias=True,\n",
    "                 start_weights_type='random'):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.weights_ = self._start_weights[start_weights_type]((out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.bias_ = self._start_weights[start_weights_type]((out_channels,)) if bias else torch.zeros((out_channels,))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f'Conv2d(in_channels={self.in_channels}, '\n",
    "                       f'out_channels={self.out_channels}, '\n",
    "                       f'kernel_size=({self.kernel_size},{self.kernel_size})')\n",
    "\n",
    "    @staticmethod\n",
    "    def _makeConvolution(X_channel, kernel):\n",
    "        k_n, k_m = kernel.shape\n",
    "        n = X_channel.shape[0] - k_n + 1\n",
    "        m = X_channel.shape[1] - k_m + 1\n",
    "        return torch.tensor([[torch.sum(X_channel[row:row+k_n, col:col+k_m]*kernel) \n",
    "                              for col in range(m)]\n",
    "                             for row in range(n)]).reshape(1,n,m)\n",
    "\n",
    "    def _getInputErrors(self, output_errors, kernel):\n",
    "        reversed_kernel = torch.flip(kernel, (0,1))\n",
    "        k_n, k_m = np.array(reversed_kernel.shape) - 1\n",
    "        n, m = output_errors.shape\n",
    "        extended_errors = torch.zeros((n + 2 * k_n, m + 2 * k_m))\n",
    "        extended_errors[k_n:-k_n, k_m:-k_m] = output_errors\n",
    "        return self._makeConvolution(extended_errors, reversed_kernel)\n",
    "\n",
    "    def _getPrevLayerErrors(self):\n",
    "        return torch.sum(torch.cat([torch.unsqueeze(torch.cat([self._getInputErrors(channel_error, kernel) \n",
    "                                                               for kernel in out_kernels], \n",
    "                                                              dim=0),\n",
    "                                                    0)\n",
    "                                    for out_kernels, channel_error in zip(self.weights_, self.errors_)]),\n",
    "                         dim=0)\n",
    "\n",
    "    def setErrors(self, errors):\n",
    "        self.errors_ = errors\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.prev_img_ = X.detach().clone()\n",
    "        return torch.cat([torch.unsqueeze(torch.sum(torch.cat([self._makeConvolution(X_channel, kernel)\n",
    "                                                               for X_channel, kernel in zip(X, out_kernels)]),\n",
    "                                                    dim=-3), \n",
    "                                          0)\n",
    "                          for out_kernels in self.weights_])\n",
    "\n",
    "    def backward(self, further_layer):\n",
    "        self.errors_ = further_layer._getPrevLayerErrors()\n",
    "\n",
    "    def step(self, lr):\n",
    "        weights_grad = torch.cat([torch.unsqueeze(torch.cat([c._makeConvolution(X_channel, channel_errors)\n",
    "                                                             for X_channel in self.prev_img_]), \n",
    "                                                  0)\n",
    "                                  for channel_errors in self.errors_])\n",
    "        self.weights_ -= lr * weights_grad\n",
    "\n",
    "    _start_weights = {'random': torch.randn}\n",
    "\n",
    "\n",
    "class UFlatten:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Flatten()'\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.initial_shape = x.shape\n",
    "        return x.reshape(self.initial_shape[0],-1)\n",
    "\n",
    "    def backward(self, further_layer):\n",
    "        self.errors_ = further_layer.errors_.reshape(self.initial_shape)\n",
    "\n",
    "    def step(self, lr):\n",
    "        pass\n",
    "\n",
    "    def _getPrevLayerErrors(self):\n",
    "        return self.errors_\n",
    "\n",
    "    def setErrors(self, errors):\n",
    "        self.errors_ = errors.reshape(self.initial_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "14c74683-3031-4b79-9ceb-470bf448b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img = torch.tensor([[[2,3,4,1,5,3,2],\n",
    "                       [6,7,3,4,6,3,2],\n",
    "                       [1,7,3,6,5,4,8],\n",
    "                       [6,3,5,0,9,6,5],\n",
    "                       [4,5,2,8,6,3,2]],\n",
    "                     \n",
    "                      [[2,3,4,1,5,3,2],\n",
    "                       [6,7,3,4,6,3,2],\n",
    "                       [1,7,3,6,5,4,8],\n",
    "                       [6,3,5,0,9,6,5],\n",
    "                       [4,5,2,8,6,3,2]]],\n",
    "                     dtype=torch.float32)\n",
    "y_img = torch.tensor([[3,4,2,3,1,2,5,4],\n",
    "                      [4,6,5,3,4,5,6,7],\n",
    "                      [1,8,3,2,9,0,7,5],\n",
    "                      [4,5,3,7,5,4,8,6]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3006e305-8c7f-4ce9-97b5-acf9faa4a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = torch.nn.Conv2d(2,3,3,bias=False)\n",
    "conv2 = torch.nn.Conv2d(3,4,2,bias=False)\n",
    "\n",
    "uconv1 = UConv2d(2,3,3,bias=False)\n",
    "uconv1.weights_ = conv1.weight.detach().clone()\n",
    "uconv2 = UConv2d(3,4,2,bias=False)\n",
    "uconv2.weights_ = conv2.weight.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c17062bd-2bf3-4632-9d64-9066b0504247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изначальные параметры\n",
      "Веса\n",
      "tensor([[[[-0.1807, -0.1847, -0.0076],\n",
      "          [ 0.2078,  0.1143, -0.2108],\n",
      "          [-0.1068, -0.1358, -0.2005]],\n",
      "\n",
      "         [[ 0.1739, -0.1954,  0.1375],\n",
      "          [ 0.2345, -0.1041,  0.1874],\n",
      "          [-0.1819,  0.2081,  0.2306]]],\n",
      "\n",
      "\n",
      "        [[[-0.0829,  0.2328,  0.0272],\n",
      "          [-0.2142,  0.1262, -0.1044],\n",
      "          [ 0.0636, -0.1200,  0.0448]],\n",
      "\n",
      "         [[ 0.1689,  0.0575,  0.1964],\n",
      "          [ 0.1837,  0.2242,  0.0451],\n",
      "          [-0.0864, -0.0614, -0.1197]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0722,  0.2158,  0.0930],\n",
      "          [ 0.2016,  0.2300,  0.0065],\n",
      "          [ 0.2292,  0.1406,  0.1186]],\n",
      "\n",
      "         [[ 0.2070,  0.2350,  0.0149],\n",
      "          [-0.1643, -0.0201,  0.0344],\n",
      "          [-0.0347, -0.1765, -0.1762]]]])\n",
      "Обновленные параметры\n",
      "Веса\n",
      "tensor([[[[  16.4318,   20.5637,   19.5545],\n",
      "          [  13.7620,   22.6163,   16.7182],\n",
      "          [  12.0723,   18.3444,   15.1953]],\n",
      "\n",
      "         [[  16.7865,   20.5530,   19.6995],\n",
      "          [  13.7887,   22.3978,   17.1165],\n",
      "          [  11.9972,   18.6883,   15.6264]]],\n",
      "\n",
      "\n",
      "        [[[-265.7108, -247.7972, -242.5377],\n",
      "          [-267.7180, -275.4545, -286.9227],\n",
      "          [-259.9064, -279.6079, -283.2387]],\n",
      "\n",
      "         [[-265.4590, -247.9725, -242.3686],\n",
      "          [-267.3202, -275.3565, -286.7732],\n",
      "          [-260.0564, -279.5493, -283.4032]]],\n",
      "\n",
      "\n",
      "        [[[-679.7573, -654.5585, -611.4039],\n",
      "          [-692.3171, -716.0521, -737.7990],\n",
      "          [-672.2595, -731.9119, -751.2924]],\n",
      "\n",
      "         [[-679.6224, -654.5394, -611.4820],\n",
      "          [-692.6830, -716.3022, -737.7711],\n",
      "          [-672.5233, -732.2290, -751.5872]]]])\n"
     ]
    }
   ],
   "source": [
    "seq_img = torch.nn.Sequential()\n",
    "seq_img.add_module('conv1', conv1)\n",
    "seq_img.add_module('conv2', conv2)\n",
    "seq_img.add_module('flatten', torch.nn.Flatten())\n",
    "\n",
    "print(f'Изначальные параметры\\nВеса\\n{seq_img[0].weight.data}')\n",
    "\n",
    "loss_function_img = torch.nn.MSELoss()\n",
    "optimizer_img = torch.optim.SGD(seq_img.parameters(), lr=0.1)\n",
    "\n",
    "for _ in range(2):\n",
    "    optimizer_img.zero_grad()\n",
    "    output_img = seq_img(X_img)\n",
    "    loss_img = loss_function_img(output_img, y_img)\n",
    "    loss_img.backward()\n",
    "    optimizer_img.step()\n",
    "\n",
    "print(f'Обновленные параметры\\nВеса\\n{seq_img[0].weight.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4c8220c2-ac85-4eb1-8ebf-54980b4b2e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изначальные параметры\n",
      "Веса\n",
      "tensor([[[[-0.1807, -0.1847, -0.0076],\n",
      "          [ 0.2078,  0.1143, -0.2108],\n",
      "          [-0.1068, -0.1358, -0.2005]],\n",
      "\n",
      "         [[ 0.1739, -0.1954,  0.1375],\n",
      "          [ 0.2345, -0.1041,  0.1874],\n",
      "          [-0.1819,  0.2081,  0.2306]]],\n",
      "\n",
      "\n",
      "        [[[-0.0829,  0.2328,  0.0272],\n",
      "          [-0.2142,  0.1262, -0.1044],\n",
      "          [ 0.0636, -0.1200,  0.0448]],\n",
      "\n",
      "         [[ 0.1689,  0.0575,  0.1964],\n",
      "          [ 0.1837,  0.2242,  0.0451],\n",
      "          [-0.0864, -0.0614, -0.1197]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0722,  0.2158,  0.0930],\n",
      "          [ 0.2016,  0.2300,  0.0065],\n",
      "          [ 0.2292,  0.1406,  0.1186]],\n",
      "\n",
      "         [[ 0.2070,  0.2350,  0.0149],\n",
      "          [-0.1643, -0.0201,  0.0344],\n",
      "          [-0.0347, -0.1765, -0.1762]]]])\n",
      "Обновленные параметры\n",
      "Веса\n",
      "tensor([[[[  16.4318,   20.5637,   19.5545],\n",
      "          [  13.7620,   22.6163,   16.7182],\n",
      "          [  12.0723,   18.3444,   15.1953]],\n",
      "\n",
      "         [[  16.7865,   20.5530,   19.6995],\n",
      "          [  13.7887,   22.3978,   17.1165],\n",
      "          [  11.9972,   18.6883,   15.6264]]],\n",
      "\n",
      "\n",
      "        [[[-265.7108, -247.7972, -242.5377],\n",
      "          [-267.7181, -275.4546, -286.9227],\n",
      "          [-259.9064, -279.6078, -283.2387]],\n",
      "\n",
      "         [[-265.4590, -247.9725, -242.3686],\n",
      "          [-267.3203, -275.3565, -286.7731],\n",
      "          [-260.0564, -279.5493, -283.4033]]],\n",
      "\n",
      "\n",
      "        [[[-679.7573, -654.5587, -611.4039],\n",
      "          [-692.3171, -716.0522, -737.7990],\n",
      "          [-672.2595, -731.9119, -751.2923]],\n",
      "\n",
      "         [[-679.6224, -654.5394, -611.4821],\n",
      "          [-692.6830, -716.3023, -737.7711],\n",
      "          [-672.5233, -732.2290, -751.5871]]]])\n"
     ]
    }
   ],
   "source": [
    "useq_img = USequential()\n",
    "useq_img.add_module('conv1', uconv1)\n",
    "useq_img.add_module('conv2', uconv2)\n",
    "useq_img.add_module('flatten', UFlatten())\n",
    "\n",
    "print(f'Изначальные параметры\\nВеса\\n{useq_img[0].weights_}')\n",
    "\n",
    "for _ in range(2):\n",
    "    uOutput = useq_img.forward(X_img)\n",
    "    useq_img.backward(uOutput, y_img, 'MSE')\n",
    "    useq_img.step(0.1)\n",
    "\n",
    "print(f'Обновленные параметры\\nВеса\\n{useq_img[0].weights_.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81061ea9-e1db-4d00-95cb-1308b8b44eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
